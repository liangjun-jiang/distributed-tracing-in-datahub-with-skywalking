---
    version: '3.5'
    services:
      mysql:
        container_name: mysql
        hostname: mysql
        image: mysql:5.7
        restart: always
        environment:
          MYSQL_DATABASE: 'datahub'
          MYSQL_USER: 'datahub'
          MYSQL_PASSWORD: 'datahub'
          MYSQL_ROOT_PASSWORD: 'datahub'
        ports:
          - "3306:3306"
        volumes:
          - /DATAHUB-ROOT/docker/mysql/init.sql:/docker-entrypoint-initdb.d/init.sql
    
      zookeeper:
        image: confluentinc/cp-zookeeper:5.2.1
        hostname: zookeeper
        container_name: zookeeper
        ports:
          - "2181:2181"
        environment:
          ZOOKEEPER_CLIENT_PORT: 2181
          ZOOKEEPER_TICK_TIME: 2000
    
      broker:
        image: confluentinc/cp-kafka:5.2.1
        hostname: broker
        container_name: broker
        depends_on:
          - zookeeper
        ports:
          - "29092:29092"
          - "9092:9092"
        environment:
          KAFKA_BROKER_ID: 1
          KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
          KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
          KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
          KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
          KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
    
      # This "container" is a workaround to pre-create topics
      kafka-setup:
        image: confluentinc/cp-kafka:5.3.0
        hostname: kafka-setup
        container_name: kafka-setup
        depends_on:
          - broker
          - schema-registry
        command: "bash -c 'echo Waiting for Kafka to be ready... && \
                           cub kafka-ready -b broker:29092 1 60 && \
                           kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 1 --replication-factor 1 --topic MetadataAuditEvent && \
                           kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 1 --replication-factor 1 --topic MetadataChangeEvent && \
                           kafka-topics --create --if-not-exists --zookeeper zookeeper:2181 --partitions 1 --replication-factor 1 --topic FailedMetadataChangeEvent'"
        environment:
          # The following settings are listed here only to satisfy the image's requirements.
          # We override the image's `command` anyways, hence this container will not start a broker.
          KAFKA_BROKER_ID: ignored
          KAFKA_ZOOKEEPER_CONNECT: ignored
    
      schema-registry:
        image: confluentinc/cp-schema-registry:5.2.1
        hostname: schema-registry
        container_name: schema-registry
        depends_on:
          - zookeeper
          - broker
        ports:
          - "8081:8081"
        environment:
          SCHEMA_REGISTRY_HOST_NAME: schema-registry
          SCHEMA_REGISTRY_KAFKASTORE_CONNECTION_URL: 'zookeeper:2181'
    
      schema-registry-ui:
        image: landoop/schema-registry-ui:latest
        container_name: schema-registry-ui
        hostname: schema-registry-ui
        ports:
          - "8000:8000"
        environment:
          SCHEMAREGISTRY_URL: 'http://schema-registry:8081'
          ALLOW_GLOBAL: 'true'
          ALLOW_TRANSITIVE: 'true'
          ALLOW_DELETION: 'true'
          READONLY_MODE: 'true'
          PROXY: 'true'
        depends_on:
          - schema-registry
    
      elasticsearch:
        image: docker.elastic.co/elasticsearch/elasticsearch:6.8.6
        container_name: elasticsearch
        hostname: elasticsearch
        ports:
          - "9200:9200"
        environment:
          - discovery.type=single-node
          - xpack.security.enabled=false
          - "ES_JAVA_OPTS=-Xms1g -Xmx1g"
    
      kibana:
        image: docker.elastic.co/kibana/kibana:6.8.6
        container_name: kibana
        hostname: kibana
        ports:
          - "5601:5601"
        depends_on:
          - elasticsearch
    
      neo4j:
        image: neo4j:3.5.7
        hostname: neo4j
        container_name: neo4j
        environment:
          NEO4J_AUTH: 'neo4j/datahub'
        ports:
          - "7474:7474"
          - "7687:7687"
    
      # This "container" is a workaround to pre-create search indices
    #   elasticsearch-setup:
    #     build:
    #       context: .
    #     hostname: elasticsearch-setup
    #     container_name: elasticsearch-setup
    #     depends_on:
    #       - elasticsearch
    #     environment:
    #       - ELASTICSEARCH_HOST=elasticsearch
    #       - ELASTICSEARCH_PORT=9200

    networks:
        default:
            name: datahub_network
